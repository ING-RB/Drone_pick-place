<?xml version="1.0"?>
<!-- Copyright 2014-2024 The MathWorks, Inc. -->
<rsccat version="1.0" locale="en_US" product="parallel">
  <message>
    <entry key="CannotAccessOutputFiles" context="error">Unable to access output files written to the folder ''{0}''. Folder location must be accessible to both the client MATLAB and the cluster machines.</entry>
    <entry key="CannotCreateOutputFolder" context="error">Failed to create the output folder ''{0}''.</entry>
    <entry key="HadoopClusterObjectNotValid" context="error">The Hadoop Cluster object is not valid. This is a result of loading the cluster object or mapreducer object from a MAT file. This is currently not supported.</entry>
    <entry key="HadoopDefaultJobName" context="uistring">MATLAB Parallel Computing Job</entry>
    <entry key="HadoopJobFailure" context="error">The HADOOP job failed to complete.</entry>
    <entry key="HadoopJobOtherFailure" context="error">The HADOOP job failed to complete. Check the HADOOP log files for job {0} for more information.</entry>
    <entry key="HadoopJobSubmitExecutionFailure" context="error">The HADOOP job failed to submit. Check for issues with the HADOOP configuration.</entry>
    <entry key="HadoopJobSubmitFailure" context="error">The HADOOP job failed to submit. This was caused by error:\n\n{0}</entry>
    <entry key="HadoopMatlabFailure" context="error">An error occurred during execution of MATLAB code:\n\n{0}</entry>
    <entry key="HadoopOutputNotFound" context="error">The HADOOP job completed successfully but the client MATLAB could not find any output files in location ''{0}''. Ensure that the ''OutputFolder'' property is set to a location that is shared by both the client machine and the Hadoop cluster. For example, a folder in HDFS or a folder on a shared network drive.</entry>
    <entry key="HadoopRequiresOutputFolder" context="error">To use Mapreduce with HADOOP, the ''OutputFolder'' parameter must specify a path to a folder that does not already exist.</entry>
    <entry key="HadoopSparkEvaluationFailure" context="error">Failed to launch MATLAB Workers on the cluster.</entry>
    <entry key="HadoopSparkJobSubmitExitEarly" context="error">Spark submit process exited early with output:\n\n{0}</entry>
    <entry key="HadoopSparkJobSubmitExitEarlyNoOutput" context="error">Spark submit process exited early with no output.</entry>
    <entry key="HadoopSparkJobSubmitFailure" context="error">Failed to launch a Spark job on the cluster. Check that the installation of Spark at ''{0}'' is valid and configured correctly.</entry>
    <entry key="HadoopSparkUnsupportedVersion" context="error">HADOOP version not supported. Evaluating tall array expressions is only supported on a HADOOP cluster of version 2 or later.</entry>
    <entry key="HadoopTaskCouldNotFindMatlab" context="error">Unable to find correct installation of MATLAB Parallel Server for attempt {0} of ''{1}'' task {2}. ''ClusterMatlabRoot'' property on the mapreducer cluster object must be set to the MATLAB Parallel Server installation on the cluster.</entry>
    <entry key="HadoopTaskInvalidHome" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2} because the HOME environment variable was set to ''{3}'', which either does not exist or is not writable. HOME environment variable must be a valid local path on the cluster. See the documentation on {4}Configure a HADOOP cluster{5}.</entry>
    <entry key="HadoopTaskOtherFailure" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2} because of an invalid license for MATLAB Parallel Server or a corruption in the MATLAB Parallel Server installation on the cluster. Check the HADOOP log files for job {3} for more information.</entry>
    <entry key="HadoopTaskVersionMismatch" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2}. HADOOP job was submitted from a client running version {3} of Parallel Computing Toolbox, to a cluster running version {4} of MATLAB Parallel Server. Client and server must run parallel computing products of the same version.</entry>
    <entry key="HadoopTaskWindowsUnsupported" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2}. Running mapreduce from MATLAB using a Hadoop cluster running on Windows is not supported.</entry>
    <entry key="InconsistentKeyType" context="error">Keys contain inconsistent types. Keys either must all be scalar numeric values of the same type or must all be character vectors.</entry>
    <entry key="InternalExecutionError" context="error">Internal error in the mapreduce framework detected during execution of mapreduce.</entry>
    <entry key="InvalidHadoopInstallFolder" context="error">Unable to recognize ''{0}'' as a valid HADOOP installation folder. Check that this is the root folder of your HADOOP installation.</entry>
    <entry key="InvalidHadoopOutputFolder" context="error">The output folder ''{0}'' could not be created by the HADOOP tasks. Check that HADOOP can access this location and that HADOOP has the permissions to create this folder.</entry>
    <entry key="InvalidKeyType" context="error">Keys contain invalid type ''{0}''. Keys either must all be scalar numeric values of the same type or must all be character vectors.</entry>
    <entry key="InvalidMapFunction" context="error">Invalid mapper function handle. This indicates the file containing the mapper function is not available on the workers. Specify the required files by setting the 'AttachedFiles' property of the pool or cluster.</entry>
    <entry key="InvalidParameterName" context="error">Invalid parameter name ''{0}''. The valid parameters for mapreduce are 'OutputFolder', 'OutputType' and 'Display'.</entry>
    <entry key="InvalidReduceFunction" context="error">Invalid reducer function handle. This indicates the file containing the mapper function is not available on the workers. Specify the required files by setting the 'AttachedFiles' property of the pool or cluster.</entry>
    <entry key="InvalidSparkInstallFolder" context="error">Unable to recognize ''{0}'' as a valid Spark installation folder. Check that this is the root folder of your Spark installation. </entry>
    <entry key="InvalidURL" context="error">Invalid URL ''{0}''. When the location is specified as a URL, it must be of the form ''scheme:/path/to/location'' or ''scheme://hostname:port/path/to/location''.</entry>
    <entry key="MismatchedKeyType" context="error">Mismatched key types returned from separate calls to the mapper function on two different workers. The types were ''{0}'' and ''{1}''.</entry>
    <entry key="MismatchedValueType" context="error">Mismatched value types returned from separate calls to the mapper function on two different workers. The types were ''{0}'' and ''{1}''.</entry>    
    <entry key="MpiShufflerAlreadyRunning" context="error">An error occurred during the shuffle-sort stage. The engine was initialized when already running.</entry>
    <entry key="MpiShufflerCancelled" context="error">An error occurred during the shuffle-sort stage on one of the other workers.</entry>
    <entry key="MpiShufflerCannotRead" context="error">An error occurred during shuffle-sort when reading from the local filesystem.</entry>    
    <entry key="MpiShufflerCannotWrite" context="error">An error occurred during shuffle-sort when writing to the local filesystem.</entry>
    <entry key="MpiShufflerCleanupError" context="error">An internal error occurred while cleaning up the shuffle-sort engine:\n{0}</entry>
    <entry key="MpiShufflerInvalidCommand" context="error">Invalid command ''{0}'' passed to the shuffle-sort engine.</entry>
    <entry key="MpiShufflerInvalidDestination" context="error">Invalid shuffle-sort destination. The destination must be the path to a writable folder.</entry>
    <entry key="MpiShufflerInvalidSource" context="error">Invalid shuffle-sort input. The input must be a cell array of filenames that is the same length as the pool size.</entry>    
    <entry key="MpiShufflerInvalidTimeout" context="error">Invalid shuffle-sort finalize timeout. This is required to be a scalar double containing a positive integer.</entry>
    <entry key="MpiShufflerNotRunning" context="error">An error occurred during the shuffle-sort stage. The engine was not initialized before sending data.</entry>
    <entry key="MpiShufflerUnknownError" context="error">Unknown internal error detected during the shuffle-sort stage.</entry>
    <entry key="ParallelHadoopMapReducerDisplayHeader" context="uistring">Parallel mapreduce execution on the Hadoop cluster. Evaluation will begin as soon as cluster resources are allocated:</entry>
    <entry key="ParallelMapReducerDisplayHeader" context="uistring">Parallel mapreduce execution on the parallel pool:</entry>
    <entry key="ParallelOutputError" context="error">An error occurred when writing the results to the output folder.</entry>
    <entry key="SparkRequiresOutputFolder" context="error">To use Mapreduce with Spark, the ''OutputFolder'' parameter must specify a path that does not already exist.</entry>
    <entry key="SpmdEnabledRequired" context="error">The specified parallel pool does not support mapreduce. Mapreduce requires a pool that has been started with its 'SpmdEnabled' property set to true.</entry>
    <entry key="StartingSparkContext" context="uistring">Starting a Spark job on the cluster. This may take a few minutes while cluster resources are allocated ...\n\n</entry>
    <entry key="StartingSparkContextDone" context="uistring">Connected to the Spark job.</entry>
    <entry key="UndefinedFunctionOnWorker" context="error">An undefined function error was thrown on the workers for ''{0}''. The file containing ''{1}'' might not be available on the workers. Specify the required files for this parallel pool using the command: addAttachedFiles(pool, ...). See the documentation for parpool for more details.</entry>
    <entry key="UndefinedFunctionOrHandleOnWorker" context="error">An undefined function error was thrown on the workers. The file containing the mapper or reducer function might not be available on the workers. Specify the required files for this parallel pool using the command: addAttachedFiles(pool, ...). See the documentation for parpool for more details.</entry>
    <entry key="UnsupportedOutputSchemeForParallelMapReducer" context="error">Scheme ''{0}'' in location ''{1}'' is not supported. Map Reduce on a parallel pool currently supports writing only to locations of the form ''file:/path/to/location''.</entry>
  </message>
</rsccat>
