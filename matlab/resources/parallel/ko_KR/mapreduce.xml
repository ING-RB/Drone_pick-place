<?xml version="1.0" encoding="UTF-8"?>
<!--Copyright 2024 The MathWorks, Inc.-->

<rsccat xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="1.0" locale="ko_KR" product="parallel" xsi:noNamespaceSchemaLocation="../../resources/schema/msgcat.xsd">
  <message>
    <entry key="CannotAccessOutputFiles" context="error">Unable to access output files written to the folder ''{0}''. Folder location must be accessible to both the client MATLAB and the cluster machines.</entry>
    <entry key="CannotCreateOutputFolder" context="error">Failed to create the output folder ''{0}''.</entry>
    <entry key="HadoopClusterObjectNotValid">The Hadoop Cluster object is not valid. This is a result of loading the cluster object or mapreducer object from a MAT file. This is currently not supported.</entry>
    <entry key="HadoopDefaultJobName">MATLAB 병렬 연산 작업</entry>
    <entry key="HadoopJobFailure">HADOOP 작업을 완료하지 못했습니다.</entry>
    <entry key="HadoopJobOtherFailure" context="error">HADOOP 작업을 완료하지 못했습니다. 자세한 내용은 작업 {0}에 대한 HADOOP 로그 파일을 확인하십시오.</entry>
    <entry key="HadoopJobSubmitExecutionFailure" context="error">The HADOOP job failed to submit. Check for issues with the HADOOP configuration.</entry>
    <entry key="HadoopJobSubmitFailure">HADOOP 작업을 제출하지 못했습니다. 오류는 다음과 같습니다.\n\n{0}</entry>
    <entry key="HadoopMatlabFailure">An error occurred during execution of MATLAB code:\n\n{0}</entry>
    <entry key="HadoopOutputNotFound">The HADOOP job completed successfully but the client MATLAB could not find any output files in location ''{0}''. Ensure that the ''OutputFolder'' property is set to a location that is shared by both the client machine and the Hadoop cluster. For example, a folder in HDFS or a folder on a shared network drive.</entry>
    <entry key="HadoopRequiresOutputFolder" context="error">To use Mapreduce with HADOOP, the ''OutputFolder'' parameter must specify a path to a folder that does not already exist.</entry>
    <entry key="HadoopSparkEvaluationFailure" context="error">Failed to launch MATLAB Workers on the cluster.</entry>
    <entry key="HadoopSparkJobSubmitExitEarly">Spark submit process exited early with output:\n\n{0}</entry>
    <entry key="HadoopSparkJobSubmitExitEarlyNoOutput" context="error">Spark submit process exited early with no output.</entry>
    <entry key="HadoopSparkJobSubmitFailure" context="error">Failed to launch a Spark job on the cluster. Check that the installation of Spark at ''{0}'' is valid and configured correctly.</entry>
    <entry key="HadoopSparkUnsupportedVersion" context="error">HADOOP version not supported. Evaluating tall array expressions is only supported on a HADOOP cluster of version 2 or later.</entry>
    <entry key="HadoopTaskCouldNotFindMatlab" context="error">Unable to find correct installation of MATLAB Parallel Server for attempt {0} of ''{1}'' task {2}. ''ClusterMatlabRoot'' property on the mapreducer cluster object must be set to the MATLAB Parallel Server installation on the cluster.</entry>
    <entry key="HadoopTaskInvalidHome" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2} because the HOME environment variable was set to ''{3}'', which either does not exist or is not writable. HOME environment variable must be a valid local path on the cluster. See the documentation on {4}Configure a HADOOP cluster{5}.</entry>
    <entry key="HadoopTaskOtherFailure" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2} because of an invalid license for MATLAB Parallel Server or a corruption in the MATLAB Parallel Server installation on the cluster. Check the HADOOP log files for job {3} for more information.</entry>
    <entry key="HadoopTaskVersionMismatch" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2}. HADOOP job was submitted from a client running version {3} of Parallel Computing Toolbox, to a cluster running version {4} of MATLAB Parallel Server. Client and server must run parallel computing products of the same version.</entry>
    <entry key="HadoopTaskWindowsUnsupported" context="error">Unable to start MATLAB for attempt {0} of ''{1}'' task {2}. Running mapreduce from MATLAB using a Hadoop cluster running on Windows is not supported.</entry>
    <entry key="InconsistentKeyType">Keys contain inconsistent types. Keys either must all be scalar numeric values of the same type or must all be character vectors.</entry>
    <entry key="InternalExecutionError">Internal error in the mapreduce framework detected during execution of mapreduce.</entry>
    <entry key="InvalidHadoopInstallFolder">''{0}''을(를) 유효한 HADOOP 설치 폴더로 인식할 수 없습니다. 설치된 HADOOP의 루트 폴더가 맞는지 확인하십시오.</entry>
    <entry key="InvalidHadoopOutputFolder">HADOOP 태스크로 출력 폴더 ''{0}''을(를) 만들 수 없습니다. HADOOP에서 이 위치에 액세스할 수 있는지와 HADOOP에 이 폴더를 만들 수 있는 권한이 있는지 확인하십시오.</entry>
    <entry key="InvalidKeyType">Keys contain invalid type ''{0}''. Keys either must all be scalar numeric values of the same type or must all be character vectors.</entry>
    <entry key="InvalidMapFunction">Invalid mapper function handle. This indicates the file containing the mapper function is not available on the workers. Specify the required files by setting the 'AttachedFiles' property of the pool or cluster.</entry>
    <entry key="InvalidParameterName">Invalid parameter name ''{0}''. The valid parameters for mapreduce are 'OutputFolder', 'OutputType' and 'Display'.</entry>
    <entry key="InvalidReduceFunction">Invalid reducer function handle. This indicates the file containing the mapper function is not available on the workers. Specify the required files by setting the 'AttachedFiles' property of the pool or cluster.</entry>
    <entry key="InvalidSparkInstallFolder">''{0}''을(를) 유효한 Spark 설치 폴더로 인식할 수 없습니다. 설치된 Spark의 루트 폴더가 맞는지 확인하십시오. </entry>
    <entry key="InvalidURL">Invalid URL ''{0}''. When the location is specified as a URL, it must be of the form ''scheme:/path/to/location'' or ''scheme://hostname:port/path/to/location''.</entry>
    <entry key="MismatchedKeyType">Mismatched key types returned from separate calls to the mapper function on two different workers. The types were ''{0}'' and ''{1}''.</entry>
    <entry key="MismatchedValueType">Mismatched value types returned from separate calls to the mapper function on two different workers. The types were ''{0}'' and ''{1}''.</entry>
    <entry key="MpiShufflerAlreadyRunning">An error occurred during the shuffle-sort stage. The engine was initialized when already running.</entry>
    <entry key="MpiShufflerCancelled">An error occurred during the shuffle-sort stage on one of the other workers.</entry>
    <entry key="MpiShufflerCannotRead">An error occurred during shuffle-sort when reading from the local filesystem.</entry>
    <entry key="MpiShufflerCannotWrite">An error occurred during shuffle-sort when writing to the local filesystem.</entry>
    <entry key="MpiShufflerCleanupError">셔플-정렬(shuffle-sort) 엔진을 정리하는 중 내부 오류가 발생했습니다.\n{0}</entry>
    <entry key="MpiShufflerInvalidCommand">Invalid command ''{0}'' passed to the shuffle-sort engine.</entry>
    <entry key="MpiShufflerInvalidDestination">Invalid shuffle-sort destination. The destination must be the path to a writable folder.</entry>
    <entry key="MpiShufflerInvalidSource">Invalid shuffle-sort input. The input must be a cell array of filenames that is the same length as the pool size.</entry>
    <entry key="MpiShufflerInvalidTimeout">Invalid shuffle-sort finalize timeout. This is required to be a scalar double containing a positive integer.</entry>
    <entry key="MpiShufflerNotRunning" context="error">셔플-정렬(shuffle-sort) 단계 중에 오류가 발생했습니다. 데이터 전송 전에 엔진이 초기화되지 않았습니다.</entry>
    <entry key="MpiShufflerUnknownError">Unknown internal error detected during the shuffle-sort stage.</entry>
    <entry key="ParallelHadoopMapReducerDisplayHeader" context="uistring">Hadoop 클러스터에서 병렬 mapreduce가 실행됩니다. 클러스터 리소스가 할당되면 바로 실행이 시작됩니다.</entry>
    <entry key="ParallelMapReducerDisplayHeader">병렬 풀에서 병렬 mapreduce 실행:</entry>
    <entry key="ParallelOutputError">출력 폴더에 결과를 쓰는 중 오류가 발생했습니다.</entry>
    <entry key="SparkRequiresOutputFolder" context="error">Spark에 Mapreduce를 사용하려면 ''OutputFolder'' 파라미터가 아직 존재하지 않는 경로를 지정해야 합니다.</entry>
    <entry key="SpmdEnabledRequired">The specified parallel pool does not support mapreduce. Mapreduce requires a pool that has been started with its 'SpmdEnabled' property set to true.</entry>
    <entry key="StartingSparkContext" context="uistring">클러스터에서 Spark 작업이 시작됩니다. 클러스터 리소스가 할당되는 동안 이 작업을 수행하는 데 몇 분 정도 소요될 수 있습니다.\n\n</entry>
    <entry key="StartingSparkContextDone" context="uistring">Spark 작업에 연결되었습니다.</entry>
    <entry key="UndefinedFunctionOnWorker">''{0}''에 대해 워커에서 정의되지 않은 함수 오류가 발생했습니다. ''{1}''이(가) 포함된 파일을 워커에서 사용하지 못할 수 있습니다. 명령 addAttachedFiles(pool, ...)을 사용하여 이 병렬 풀에 대한 필수 파일을 지정하십시오. 자세한 내용은 parpool에 대한 문서를 참조하십시오.</entry>
    <entry key="UndefinedFunctionOrHandleOnWorker">An undefined function error was thrown on the workers.  The file containing the mapper or reducer function might not be available on the workers.  Specify the required files for this parallel pool using the command: addAttachedFiles(pool, ...).  See the documentation for parpool for more details.</entry>
    <entry key="UnsupportedOutputSchemeForParallelMapReducer" context="error">Scheme ''{0}'' in location ''{1}'' is not supported. Map Reduce on a parallel pool currently supports writing only to locations of the form ''file:/path/to/location''.</entry>
  </message>
</rsccat>
