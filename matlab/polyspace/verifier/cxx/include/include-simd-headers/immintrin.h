/* Copyright 2024 The MathWorks, Inc. */
#include <stdint.h>
typedef void* __m256;
typedef void* __m256i;
typedef void* __m256d;
typedef void* __m128;
typedef void* __m128i;
typedef void* __m128d;
typedef void* __m512;
typedef void* __m512i;
typedef void* __m512d;
typedef void* __mmask8;
typedef void* __mmask16;
typedef float single;
typedef int8_t int8;
typedef int16_t int16;
typedef int32_t int32;
typedef int64_t int64;
typedef uint32_t uint32;

//AVX --> immintrin.h
__m256d   _mm256_add_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_and_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_blendv_pd( __m256d u2, __m256d u3, __m256d u1  );
__m256d   _mm256_ceil_pd( __m256d u1  );
__m256d   _mm256_cmp_pd( __m256d u1, __m256d u2, int8 u3  );
__m256d   _mm256_cvtps_pd( __m256 u1  );
__m256d   _mm256_cvtps_pd( __m128 u1  );
__m256d   _mm256_div_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_floor_pd( __m256d u1  );
__m256d   _mm256_loadu_pd( const double* u1  );
__m256d   _mm256_max_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_min_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_mul_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_or_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_permute4x64_pd( __m256d u1, uint32 u4  );
__m256d   _mm256_set_m128d( __m128d u1, __m128d u2  );
__m256d   _mm256_set1_pd( double u1  );
__m256d   _mm256_set_pd( double u1, double u2, double u3, double u4  );
__m256d   _mm256_shuffle_pd( __m256d u1, __m256d u2, uint32 u3  );
__m256d   _mm256_sqrt_pd( __m256d u1  );
__m256d   _mm256_sub_pd( __m256d u1, __m256d u2  );
__m256d   _mm256_xor_pd( __m256d u1, __m256d u2  );
__m256i   _mm256_loadu_si256( const __m256i* u1  );
__m256i   _mm256_set1_epi16( int16 u1  );
__m256i   _mm256_set1_epi32( int32 u1  );
__m256i   _mm256_set1_epi64x( int64 u1  );
__m256i   _mm256_set1_epi8( int8 u1  );
__m256i   _mm256_set_epi16( int16 u1, int16 u2, int16 u3, int16 u4, int16 u5, int16 u6, int16 u7, int16 u8, int16 u9, int16 u10, int16 u11, int16 u12, int16 u13, int16 u14, int16 u15, int16 u16  );
__m256i   _mm256_set_epi32( int32 u1, int32 u2, int32 u3, int32 u4, int32 u5, int32 u6, int32 u7, int32 u8  );
__m256i   _mm256_set_epi64x( int64 u1, int64 u2, int64 u3, int64 u4  );
__m256i   _mm256_set_epi8( int8 u1, int8 u2, int8 u3, int8 u4, int8 u5, int8 u6, int8 u7, int8 u8, int8 u9, int8 u10, int8 u11, int8 u12, int8 u13, int8 u14, int8 u15, int8 u16, int8 u17, int8 u18, int8 u19, int8 u20, int8 u21, int8 u22, int8 u23, int8 u24, int8 u25, int8 u26, int8 u27, int8 u28, int8 u29, int8 u30, int8 u31, int8 u32  );
__m256i   _mm256_set_m128i( __m128i u1, __m128i u2  );
__m256   _mm256_add_ps( __m256 u1, __m256 u2  );
__m256   _mm256_and_ps( __m256 u1, __m256 u2  );
__m256   _mm256_blendv_ps( __m256 u2, __m256 u3, __m256 u1  );
__m256   _mm256_castsi256_ps( __m256i u1  );
__m256   _mm256_ceil_ps( __m256 u1  );
__m256   _mm256_cmp_ps( __m256 u1, __m256 u2, int8 u3  );
__m256   _mm256_cvtpd_ps( __m256d u1  );
__m256   _mm256_div_ps( __m256 u1, __m256 u2  );
__m256   _mm256_floor_ps( __m256 u1  );
__m256 _mm256_loadu_ps( const single* u1  );
__m256   _mm256_max_ps( __m256 u1, __m256 u2  );
__m256   _mm256_min_ps( __m256 u1, __m256 u2  );
__m256   _mm256_mul_ps( __m256 u1, __m256 u2  );
__m256   _mm256_or_ps( __m256 u1, __m256 u2  );
__m256   _mm256_permutevar8x32_ps( __m256 u1, __m256i u3  );
__m256   _mm256_set1_ps( single u1  );
__m256   _mm256_set_ps( single u1, single u2, single u3, single u4, single u5, single u6, single u7, single u8  );
__m256   _mm256_set_m128( __m256 u1, __m256 u2  );
__m256   _mm256_shuffle_ps( __m256 u1, __m256 u2, uint32 u3  );
__m256   _mm256_sqrt_ps( __m256 u1  );
__m256   _mm256_sub_ps( __m256 u1, __m256 u2  );
__m256   _mm256_xor_ps( __m256 u1, __m256 u2  );
void _mm256_storeu_pd( double* u1, __m256d u2  );
void _mm256_storeu_ps( single* u1, __m256 u2  );
void _mm256_storeu_si256( __m256i* u1, __m256i u2  );

//AVX2 --> immintrin.h
__m128   _mm256_extractf128_ps( __m256 u1, int32 u2  );
__m128   _mm256_cvtpd_ps( __m256d u1  );
__m128d   _mm256_extractf128_pd( __m256d u1, int32 u2  );
__m128i   _mm_sllv_epi32( __m128i u1, __m128i u2  );
__m128i   _mm_sllv_epi64( __m128i u1, __m128i u2  );
__m128i   _mm_srav_epi32( __m128i u1, __m128i u2  );
__m128i   _mm256_extracti128_si256( __m256i u1, int32 u2  );
__m256i   _mm256_add_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_add_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_add_epi64( __m256i u1, __m256i u2  );
__m256i   _mm256_add_epi8( __m256i u1, __m256i u2  );
__m256i   _mm256_and_si256( __m256i u1, __m256i u2  );
__m256i   _mm256_blendv_epi8( __m256i u2, __m256i u3, __m256i u1  );
__m256i   _mm256_cmpeq_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpeq_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpeq_epi64( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpeq_epi8( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpgt_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpgt_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpgt_epi64( __m256i u1, __m256i u2  );
__m256i   _mm256_cmpgt_epi8( __m256i u1, __m256i u2  );
__m256i   _mm256_cvtepi16_epi32( __m128i u1  );
__m256i   _mm256_cvtepi16_epi64( __m128i u1  );
__m256i   _mm256_cvtepi32_epi64( __m128i u1  );
__m256i   _mm256_cvtepi8_epi16( __m128i u1  );
__m256i   _mm256_cvtepi8_epi32( __m128i u1  );
__m256i   _mm256_cvtepi8_epi64( __m128i u1  );
__m256i   _mm256_cvtepu16_epi32( __m128i u1  );
__m256i   _mm256_cvtepu16_epi64( __m128i u1  );
__m256i   _mm256_cvtepu32_epi64( __m128i u1  );
__m256i   _mm256_cvtepu8_epi16( __m128i u1  );
__m256i   _mm256_cvtepu8_epi32( __m128i u1  );
__m256i   _mm256_cvtepu8_epi64( __m128i u1  );
__m256i   _mm256_cvtps_epi32( __m256 u1  );
__m256i   _mm256_cvttps_epi32( __m256 u1  );
__m256d   _mm256_cvtepi32_pd( __m128i u1  );
__m128i   _mm256_cvtpd_epi32( __m256d u1  );
__m128i   _mm256_cvttpd_epi32( __m256d u1  );
__m256   _mm256_cvtepi32_ps( __m256i u1  );
__m256i   _mm256_max_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_max_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_max_epi8( __m256i u1, __m256i u2  );
__m256i   _mm256_min_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_min_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_min_epi8( __m256i u1, __m256i u2  );
__m256i   _mm256_mullo_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_mullo_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_or_si256( __m256i u1, __m256i u2  );
__m256i   _mm256_sllv_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_sllv_epi64( __m256i u1, __m256i u2  );
__m256i   _mm256_srav_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_sub_epi16( __m256i u1, __m256i u2  );
__m256i   _mm256_sub_epi32( __m256i u1, __m256i u2  );
__m256i   _mm256_sub_epi64( __m256i u1, __m256i u2  );
__m256i   _mm256_sub_epi8( __m256i u1, __m256i u2  );
__m256i   _mm256_xor_si256( __m256i u1, __m256i u2  );

//AVX515F
__m128i   _mm_cvtepi32_epi16( __m128i u1  );
__m128i   _mm_cvtepi32_epi8( __m128i u1  );
__m128i   _mm_cvtepi64_epi16( __m128i u1  );
__m128i   _mm_cvtepi64_epi32( __m128i u1  );
__m128i   _mm_cvtepi64_epi8( __m128i u1  );
__m128i   _mm512_cvtepi32_epi8( __m512i u1  );
__m128i   _mm512_cvtepi64_epi16( __m512i u1  );
__m256   _mm512_cvtpd_ps( __m512d u1  );
__m512d   _mm512_add_pd( __m512d u1, __m512d u2  );
__m512d   _mm512_cvtps_pd( __m256 u1  );
__m512d   _mm512_cvtepi32_pd( __m256i u1  );
__m512d   _mm512_div_pd( __m512d u1, __m512d u2  );
__m512d   _mm512_fmadd_pd( __m512d u1, __m512d u2, __m512d u3  );
__m512d   _mm512_fmsub_pd( __m512d u1, __m512d u2, __m512d u3  );
__m512d   _mm512_loadu_pd( const double* u1  );
__m512d   _mm512_mask_blend_pd( __mmask8 u1, __m512d u2, __m512d u3  );
__m512d   _mm512_max_pd( __m512d u1, __m512d u2  );
__m512d   _mm512_min_pd( __m512d u1, __m512d u2  );
__m512d   _mm512_mul_pd( __m512d u1, __m512d u2  );
__m512d   _mm512_permutexvar_pd( __m512i u3, __m512d u1  );
__m512d   _mm512_set1_pd( double u1  );
__m512d   _mm512_set_pd( double u1, double u2, double u3, double u4, double u5, double u6, double u7, double u8  );
__m512d   _mm512_shuffle_pd( __m512d u1, __m512d u2, uint32 u3  );
__m512d   _mm512_sqrt_pd( __m512d u1  );
__m512d   _mm512_sub_pd( __m512d u1, __m512d u2  );
__m512i   _mm512_add_epi32( __m512i u1, __m512i u2  );
__m512i   _mm512_add_epi64( __m512i u1, __m512i u2  );
__m512i   _mm512_and_si512( __m512i u1, __m512i u2  );
__m512i   _mm512_castps_si512( __m512 u1  );
__m512i   _mm512_cvtepi16_epi32( __m256i u1  );
__m512i   _mm512_cvtepi16_epi64( __m128i u1  );
__m512i   _mm512_cvtepi32_epi64( __m256i u1  );
__m512i   _mm512_cvtepi8_epi32( __m128i u1  );
__m512i   _mm512_cvtepi8_epi64( __m128i u1  );
__m512i   _mm512_cvtepu16_epi32( __m256i u1  );
__m512i   _mm512_cvtepu16_epi64( __m128i u1  );
__m512i   _mm512_cvtepu32_epi64( __m256i u1  );
__m512i   _mm512_cvtepu8_epi32( __m128i u1  );
__m512i   _mm512_cvtepu8_epi64( __m128i u1  );
__m512i   _mm512_cvtps_epi32( __m512 u1  );
__m512i   _mm512_cvttps_epi32( __m512 u1  );
__m512i   _mm512_loadu_si512( void* u1  );
__m512i   _mm512_mask_blend_epi32( __mmask16 u1, __m512i u2, __m512i u3  );
__m512i   _mm512_mask_blend_epi64( __mmask8 u1, __m512i u2, __m512i u3  );
__m512i   _mm512_max_epi32( __m512i u1, __m512i u2  );
__m512i   _mm512_max_epi64( __m512i u1, __m512i u2  );
__m512i   _mm512_min_epi32( __m512i u1, __m512i u2  );
__m512i   _mm512_min_epi64( __m512i u1, __m512i u2  );
__m512i   _mm512_or_si512( __m512i u1, __m512i u2  );
__m512i   _mm512_set1_epi16( int16 u1  );
__m512i   _mm512_set1_epi32( int32 u1  );
__m512i   _mm512_set1_epi64( int64 u1  );
__m512i   _mm512_set1_epi8( int8 u1  );
__m512i   _mm512_set_epi16( int16 u1, int16 u2, int16 u3, int16 u4, int16 u5, int16 u6, int16 u7, int16 u8, int16 u9, int16 u10, int16 u11, int16 u12, int16 u13, int16 u14, int16 u15, int16 u16, int16 u17, int16 u18, int16 u19, int16 u20, int16 u21, int16 u22, int16 u23, int16 u24, int16 u25, int16 u26, int16 u27, int16 u28, int16 u29, int16 u30, int16 u31, int16 u32  );
__m512i   _mm512_set_epi32( int32 u1, int32 u2, int32 u3, int32 u4, int32 u5, int32 u6, int32 u7, int32 u8, int32 u9, int32 u10, int32 u11, int32 u12, int32 u13, int32 u14, int32 u15, int32 u16  );
__m512i   _mm512_set_epi64( int64 u1, int64 u2, int64 u3, int64 u4, int64 u5, int64 u6, int64 u7, int64 u8  );
__m512i   _mm512_set_epi8( int8 u1, int8 u2, int8 u3, int8 u4, int8 u5, int8 u6, int8 u7, int8 u8, int8 u9, int8 u10, int8 u11, int8 u12, int8 u13, int8 u14, int8 u15, int8 u16, int8 u17, int8 u18, int8 u19, int8 u20, int8 u21, int8 u22, int8 u23, int8 u24, int8 u25, int8 u26, int8 u27, int8 u28, int8 u29, int8 u30, int8 u31, int8 u32, int8 u33, int8 u34, int8 u35, int8 u36, int8 u37, int8 u38, int8 u39, int8 u40, int8 u41, int8 u42, int8 u43, int8 u44, int8 u45, int8 u46, int8 u47, int8 u48, int8 u49, int8 u50, int8 u51, int8 u52, int8 u53, int8 u54, int8 u55, int8 u56, int8 u57, int8 u58, int8 u59, int8 u60, int8 u61, int8 u62, int8 u63, int8 u64  );
__m512i   _mm512_sllv_epi32( __m512i u1, __m512i u2  );
__m512i   _mm512_sllv_epi64( __m512i u1, __m512i u2  );
__m512i   _mm512_srav_epi32( __m512i u1, __m512i u2  );
__m512i   _mm512_srav_epi64( __m512i u1, __m512i u2  );
__m512i   _mm512_sub_epi32( __m512i u1, __m512i u2  );
__m512i   _mm512_sub_epi64( __m512i u1, __m512i u2  );
__m512i   _mm512_xor_si512( __m512i u1, __m512i u2  );
__m512   _mm512_add_ps( __m512 u1, __m512 u2  );
__m512   _mm512_castsi512_ps( __m512i u1  );
__m512   _mm512_cvtepi32_ps( __m512i u1  );
__m512   _mm512_div_ps( __m512 u1, __m512 u2  );
__m512   _mm512_fmadd_ps( __m512 u1, __m512 u2, __m512 u3  );
__m512   _mm512_fmsub_ps( __m512 u1, __m512 u2, __m512 u3  );
__m512   _mm512_loadu_ps( const single* u1  );
__m512   _mm512_mask_blend_ps( __mmask16 u1, __m512 u2, __m512 u3  );
__m512   _mm512_max_ps( __m512 u1, __m512 u2  );
__m512   _mm512_min_ps( __m512 u1, __m512 u2  );
__m512   _mm512_mul_ps( __m512 u1, __m512 u2  );
__m512   _mm512_permutexvar_ps( __m512i u3, __m512 u1  );
__m512   _mm512_set1_ps( single u1  );
__m512   _mm512_set_ps( single u1, single u2, single u3, single u4, single u5, single u6, single u7, single u8, single u9, single u10, single u11, single u12, single u13, single u14, single u15, single u16  );
__m512   _mm512_shuffle_ps( __m512 u1, __m512 u2, uint32 u3  );
__m512   _mm512_sqrt_ps( __m512 u1  );
__m512   _mm512_sub_ps( __m512 u1, __m512 u2  );
void _mm512_storeu_pd( double* u1, __m512d u2  );
void _mm512_storeu_ps( single* u1, __m512 u2  );
void _mm512_storeu_si512( void* u1, __m512i u2  );
__mmask8   _mm512_cmp_pd_mask( __m512d u1, __m512d u2, int8 u3  );
__mmask8   _mm512_cmpeq_epi64_mask( __m512i u1, __m512i u2  );
__mmask8   _mm512_cmpge_epi64_mask( __m512i u1, __m512i u2  );
__mmask8   _mm512_cmpgt_epi64_mask( __m512i u1, __m512i u2  );
__mmask8   _mm512_cmple_epi64_mask( __m512i u1, __m512i u2  );
__mmask8   _mm512_cmplt_epi64_mask( __m512i u1, __m512i u2  );
__mmask16   _kand_mask16( __mmask16 u1, __mmask16 u2  );
__mmask16   _kor_mask16( __mmask16 u1, __mmask16 u2  );
__mmask16   _kxor_mask16( __mmask16 u1, __mmask16 u2  );
__mmask16   _mm512_cmp_ps_mask( __m512 u1, __m512 u2, int8 u3  );
__mmask16   _mm512_cmpeq_epi32_mask( __m512i u1, __m512i u2  );
__mmask16   _mm512_cmpge_epi32_mask( __m512i u1, __m512i u2  );
__mmask16   _mm512_cmpgt_epi32_mask( __m512i u1, __m512i u2  );
__mmask16   _mm512_cmple_epi32_mask( __m512i u1, __m512i u2  );
__mmask16   _mm512_cmplt_epi32_mask( __m512i u1, __m512i u2  );

__m256   _mm512_extractf32x8_ps( __m512 u1, int32 u2  );
__m256d   _mm512_extractf64x4_pd( __m512d u1, int32 u2  );
__m256i   _mm512_extracti64x4_epi64( __m512i u1, int32 u2  );
__m256i   _mm512_cvtepi64_epi32( __m512i u1  );
__m256i   _mm512_cvtpd_epi32( __m512d u1  );
__m256i   _mm512_cvttpd_epi32( __m512d u1  );
__m256i   _mm512_cvtepi32_epi16( __m512i u1  );

//FMA
__m256   _mm256_fmadd_ps( __m256 u1, __m256 u2, __m256 u3  );
__m256d   _mm256_fmadd_pd( __m256d u1, __m256d u2, __m256d u3  );
__m256   _mm256_fmsub_ps( __m256 u1, __m256 u2, __m256 u3  );
__m256d   _mm256_fmsub_pd( __m256d u1, __m256d u2, __m256d u3  );
